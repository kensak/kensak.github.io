<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[AIへ秒読み]]></title>
  <link href="http://kensak.github.io/atom.xml" rel="self"/>
  <link href="http://kensak.github.io/"/>
  <updated>2014-02-12T14:42:29+09:00</updated>
  <id>http://kensak.github.io/</id>
  <author>
    <name><![CDATA[榊原 研]]></name>
    <email><![CDATA[ken.sakakibar@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[脳のコネクショニスト・モデル]]></title>
    <link href="http://kensak.github.io/blog/2014/02/12/connectionist-brain-model/"/>
    <updated>2014-02-12T14:30:00+09:00</updated>
    <id>http://kensak.github.io/blog/2014/02/12/connectionist-brain-model</id>
    <content type="html"><![CDATA[<p><a href="https://staff.aist.go.jp/y-ichisugi/j-index.html">産業技術総合研究所の一杉裕志氏のサイト</a>で
氏は大脳皮質の計算論的モデル BESOM を提案しています。
それは４つの機械学習技術（自己組織化マップ、ベイジアンネット、独立成分分析、強化学習）を
組み合わせたもので、
すでにプログラムがありソースコードも公開されています。
その機能はもちろんまだ実際の人間の脳の機能からは程遠いとはいえ、
ここまで具体的な脳のモデルを既知の技術の組み合わせにより提示できるということが、
機械学習の進歩を物語っていると思います。</p>

<p>実際の脳にできることを考えてみると、脳の機能をコンピューター上で実現しようとするときの必要条件が見えてきます。</p>

<p>例えば</p>

<ul>
  <li>ニューロンの活動（化学的な反応）の遅さにも関わらす
高速な処理が可能　→　大規模な並列計算が必要</li>
  <li>人間の赤ん坊はさまざまな映像の中から一塊の物体や親の顔を認識するようになる
→ 独立成分分析のような「抽象化」をおこなう仕組みが必要</li>
  <li>快を求め不快を避けるという動物に共通した性質を実現するには強化学習のような
 「報酬」という概念を含む機構が必要</li>
</ul>

<p>ここで思考実験として、脳のコネクショニスト（つまりニューラル・ネットを用いた）モデルを
人間の脳が持つ能力から逆算しながら構成するということを試みたいと思います。
つまり、知能の個体発生（認知の発達過程）をたどりながら、人間の脳が備えている能力と、
それをニューラル・ネットの手法によって実現する方法を考えていきます。</p>

<!-- more -->

<h3 id="section">分節・抽象化</h3>
<p>人間の赤ん坊は生れたときは目が悪く、明暗しか分かりません。
遠近の違いや立体なども認識することができません。
それが、いろいろな物や景色を見る体験を通して、形が認識できるようになってきます。
つまり物の形を周囲から切り離して（分節して）認識できるようになるわけです。</p>

<p>また、2ヶ月を過ぎたころからパパとママに対して別の反応をするようにもなります。
つまり、パパの顔の映像を、心の中にある「パパ」という観念に結び付けて認識することができるようになるわけです。
これは、時によって見え方が異なるパパの顔の映像から「パパ」という観念を抽象する操作とも言えます。</p>

<p>パワン・シンハは <a href="http://www.ted.com/talks/pawan_sinha_on_how_brains_learn_to_see.html">TED での講演</a>
の中で、生まれつき目の見えない子供たちに視力回復治療をおこなうプロジェクトについて話しています。
子供たちは手術によって外部からの光の情報を網膜を通して受け取れるようになりますが、それだけでは
実世界の物を認識できるようにはなりません。彼らには世界が色や明るさの違う無数の細かい領域に分かれている
ように見えていて、例えばボールの影の部分が一つの物として見えてしまうし、四角と丸が重なった図形を
見せると三つの物があると答える。
しかし数ヶ月たつと、丸を丸、四角を四角として、そして現実世界にある物（人、牛、……）を
それぞれ一つの物として認識できるようになる。これは、それらの物が一つのまとまりとして動く様を
何度も見ることによって、脳が正しい分節を学習することによります。</p>

<p>分節と抽象化は情報の圧縮という点で一つにくくることができます。現実によく出現するまとまりが一つの
物として扱われるよううまく分節すると、あらゆる現実のパターンを効率よく表現することができます。
また、分節化された個々のまとまりにコンパクトな表現（例えばベクトル表現など）を与え、
高次の処理をおこなうときのための記号とすることが抽象化です。</p>

<p>ここでは autoencoder を使ってこの情報の圧縮をモデル化します。
autoencoder を使えば、情報の圧縮を教師なしで、また SGD と組み合わせてオンラインでおこなうことができます。</p>

<p>知覚による入力 $x$ を受けて、その「表現」あるいは「表象」（representation）を返す関数 A を
「抽象器（Abstractor）」と呼ぶことにします。</p>

<script type="math/tex; mode=display">
y = \mathrm{A}(x)
</script>

<p>また、その逆方向の関数 $\mathrm{A’}$ を一つ固定しておきます。</p>

<p>分節・抽象化の学習は、いろいろな入力 $x$ に対し、$x’=\mathrm{A’}(\mathrm{A}(x))$ として $ ||x-x’|| $ が小さくなる
ように $\mathrm{A}, \mathrm{A’}$ を変更していくことに相当します。</p>

<h3 id="section-1">運動の学習</h3>
<p>やがて赤ん坊は体を動かすことを覚えます。
寝返りさえできない状態から、寝返りし、ハイハイし、立ち上がり、
歩き、ということができるようになっていく過程で、
赤ん坊は体や筋肉をどのように動かすといいのかを学んでいきます。
同時に、口の形を変えたり息を調節したりして音を出すことも学習します。</p>

<p>ある表象 $y$ を受けて「運動」を返す関数 $\mathrm{C}$ を
「具象器（Concretizer）」と呼ぶことにします。</p>

<p>また、ある運動 $z$ をおこなったときに、それに対し、その結果として得られる知覚を
返す関数 $\mathrm{W}$ を「外部世界（World）」と呼ぶことにします。</p>

<p>運動の学習の捉え方はいろいろあると思いますが、ここではこのような場面を想定します。
3ヶ月ぐらいの赤ん坊に親が話しかけると、赤ん坊はそれに反応して片言を発します。
やがて赤ん坊は声を真似するようになり、まず母音が、次に子音が話せるようになり、
8ヶ月ぐらいで「ママ」などと言い始めます。</p>

<p>母親の声を $x$ として、それに対する赤ん坊の頭の中の表象を $y=\mathrm{A}(x)$、
それによる赤ん坊の口の運動を $z=\mathrm{C}(y)$、
それが引き起こす知覚（つまり赤ん坊が聞く自分の声）を $x’=\mathrm{W}(z)$、
それが惹起する表象を $y’=\mathrm{A}(x’)$ とします。</p>

<p>運動の学習は、いろいろな入力 $x$ に対し、$y’=\mathrm{A}(\mathrm{W}(\mathrm{C}(\mathrm{A}(x))))$ として $||y-y’||$ が小さくなる
ように $\mathrm{C}$ を変更していくことに相当します。</p>

<h3 id="section-2">外部世界モデル</h3>
<p>人間は常に今から少しあとに起きることを予想しながら生きています。
例えば道を歩くときも、路面の状態や周りの状況を見ながら一瞬あとを予想し、
それに基づいて次に足を置く位置や歩く速さを調整しています。
人と話しているときには相手の表情や声の調子から一瞬ごとに相手の次の出方を予測し、
それによって次に話す内容や話す調子を決めています。</p>

<p>また、人間は外部からの刺激がなくてもいろいろな状況を自分で想像することがあります。
犬小屋を作る前に頭の中で必要な作業について考えてみたり、
明日会うはずの人に対して、こう言ってみたら何と答えるだろう、などと考えてみたりします。</p>

<p>こういったことから、人間の脳内には外部の世界についてのあるモデルがあると考えられます。
それは大雑把に言えば、人なら誰しも頭の中に持っている、
外部の世界についての「世界とはこんなものだ」とか
「もしこうしたらこうなるだろう」というような考えすべての寄せ集めのことです。</p>

<p>人はいろいろな経験をしながらこの世界についてのモデルを修正していき、
これからの予測のために役立てます。
ジェフ・ホーキンスはその<a href="http://www.ted.com/talks/jeff_hawkins_on_how_brain_science_will_change_computing.html">講演</a>の中で、
予測の能力ことが知能の本質だとして重視しています。</p>

<p>ある意志の表象を $y$ として、それに対し、その意志を運動にしたときに
外部世界から返ってくる知覚の予想 $y’$ を返す関数 $\mathrm{W_m}$ を
「外部世界モデル」と呼ぶことにします。</p>

<p>外部世界モデルの学習は、いろいろな意志の表象 $y_1$ に対し、
$ y_1=\mathrm{A}(\mathrm{W}(\mathrm{C}(y))), y_2=\mathrm{W_m}(y) $ としたときに $ ||y_1-y_2|| $ が小さくなる
ように $\mathrm{W_m}$ を変更していくことに相当します。</p>

<h3 id="section-3">報酬の最大化</h3>
<p>快を求め不快を避けるという動物に共通する傾向のモデルとしては強化学習が有名ですが、
ここではあくまでもニューラル・ネットにこだわってみたいと思います。</p>

<p>ある表象 $y$ を、別の表象 $y’$ と「報酬（reward）」$r$ の組に写す関数 $\mathrm{T}$ を「思考器（Thinker）」と呼ぶことにします。</p>

<script type="math/tex; mode=display">
(y', r) = \mathrm{T}(y)
</script>

<p>ここで、人間ではある種の知覚（や表象）とある種の情動（例えば恐怖など）が固定的に結びつけられていることを考えると、
$\mathrm{T}$ のパラメターの一部は固定されているのかもしれません。</p>

<p>「思考器」というときの「思考」は広い意味で捉えています。いわゆる「感じる」ことも含まれます。
また、考えるという行為は思考器だけでおこなうのではなく、外部世界モデルも使います。
思考器と外部世界モデルの間を表象がいったりきたりすることで考える、ということをイメージしています。
考えるためのモジュールから外部世界モデルだけを切り離したのは、そこだけで別に学習できるのでは、と思ったからです。</p>

<p>一方で、思考という行為の複雑さから考えると、$\mathrm{T}$ はそれ自体の中にループを含む、
recurrent なネットワークになっているのかもしれません。</p>

<p>思考器の学習は、いろいろな表象入力 $y$ に対し、
$(y’, r) = \mathrm{T}(y)$ で得られる報酬 $r$ を大きくするように $\mathrm{T}$ を変更していくことに相当します。</p>

<p>また、報酬を最大化するための更新は $\mathrm{T}$ にとどまらず $\mathrm{W_m}$ に対してもおこなわれる、と考えることもできます。</p>

<p>これをまとめると以下の図のようになります。
<img src="http://kensak.github.io/assets/NN_model.png" alt="NN brain model" /></p>

<p>$\mathrm{T}$ は初期状態では恒等写像とします。</p>

<p>モジュールの更新はだいたい $\mathrm{A}, \mathrm{C}, \mathrm{W_m}, \mathrm{T}$ の順番でおこないますが、厳密ではありません。
赤ん坊は外部世界について学習しながら運動することも同時に学びます。
大人になってから外国語を学ぶとき、聞き取りない音が聞き取れるようになるには $\mathrm{A}$ 
の再学習が必要になりますし、また話すためには $\mathrm{C}$ が更新されるはずです。
また、一般に思考器と外部世界モデルはお互いに影響し合いながら同時に更新されていくでしょう。
もちろん大人よりも子供や赤ん坊の方がより学習が強くおこなわれ、柔軟に周囲の環境に適応
することができるのは確かです。</p>

<p>本能のような最初から固定されている部分、
ある年齢まではさかんに学習をおこなうがそれ以降はあまり更新されない部分、
などの違いを可能にするためには、学習の強さやその時間的な制限条件を決める追加のパラメターが必要となるでしょう。</p>

<p>このモデルでは、人間は外部世界と合わせて一つの recurrent なネットワークを形成しています。
知覚した情報を分節化することを学びながら運動することを学習し、
外部世界についての考えを深めながら自己実現（報酬の最大化）を学ぶ、というように、
このネットワークはさまざまな刺激を得ながら各モジュールを同時に更新していきます。
この EM 的な更新と recurrent 性により、遠い因果関係をたどったり、はるか未来に受け取るはずの報酬を最大化するなど、
各モジュールにおいて複雑な関数を学習できる可能性があります。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ランダムな置換とクヌース・シャッフル]]></title>
    <link href="http://kensak.github.io/blog/2014/01/13/knuth-shuffle/"/>
    <updated>2014-01-13T19:52:19+09:00</updated>
    <id>http://kensak.github.io/blog/2014/01/13/knuth-shuffle</id>
    <content type="html"><![CDATA[<p>ニューラルネットをプログラムするときに、ランダムな置換
（ここでは $ 0 $ から $ n-1 $ までの数がランダムな順で並んだものと考えます）が
必要になることがあります。例えば、</p>

<ul>
  <li>SGD with minibatch において、サンプルの配列の中からランダムな順序で 100 ずつ、
順番に取り出したい。ランダムな置換を使って配列のインデックスをかき混ぜればよい。</li>
  <li>Dropout において、n 個の入力のうちランダムに選んだ半分を 0 にセットしたい。
それぞれの入力を確率 0.5 で 0 にするのではちょうど半分になるとは限らない。
ランダムな置換を使えば、$ n / 2 $ 未満の要素を選ぶことによりちょうど半分
をランダムに選ぶことができる。半分ではなく他の比率でも大丈夫。</li>
</ul>

<p>ランダムな置換を得るには <a href="http://en.wikipedia.org/wiki/Random_permutation">Knuth shuffles</a>
という便利なアルゴリズムがあります。これは $ 0, 1, \ldots, n - 1 $ という
シーケンスに対し、$i$ 番目と、$ 0, \ldots, i $ からランダムに選んだ $j$
による $j$ 番目とを入れ替える、という操作を $i = 0, 1, \ldots, n-1$ について
繰り返すというものです。</p>

<p>変数の初期化も同時にやってしまうのが以下のコードです。</p>

<!-- more -->

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
</pre></td><td class="code"><pre><code class="cpp"><span class="line"><span class="c1">// get a random permutation of [0, 1, .., n-1] where n = vec.size().</span>
</span><span class="line"><span class="kt">void</span> <span class="n">GetRandomPermutation</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">unsigned</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">vec</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">  <span class="c1">// Knuth shuffles algorithm</span>
</span><span class="line">  <span class="k">const</span> <span class="n">size_t</span> <span class="n">n</span> <span class="o">=</span> <span class="n">vec</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
</span><span class="line">  <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
</span><span class="line">  <span class="p">{</span>
</span><span class="line">    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
</span><span class="line">    <span class="n">vec</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">vec</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
</span><span class="line">    <span class="n">vec</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
</span><span class="line">  <span class="p">}</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>ここで <code>uniform(0, i)</code> は $0, 1, \ldots,i$ の中から同確率で一つ選んで
返す関数です。</p>

<p>9 行目で $i = j$ の場合、まだ初期化されていない値を自分に代入することになりますが、
そのときは次の 10 行目で $i$ が上書きされるので問題ありません。</p>

<p>このように簡単なアルゴリズムですが、
これで最終的に得られる置換の分布は一様であることが、次のように証明できます。</p>

<p>数学的帰納法を使います。  </p>

<p>「$k$ 番目のループ（$ k = 0, 1, \ldots, n-1 $）のあと、$ \mathrm{vec}[p]~~(p \in [0,k]) $ の値が $q \in [0, k]$ である確率は $p, q$
に関わらずすべて $1 / (k + 1)$ である」</p>

<p>という命題を $ \mathrm{Prop}(k) $ とします。証明したいのは $ \mathrm{Prop}(n-1) $ です。<br />
$ \mathrm{Prop}(0) $ は自明です。<br />
$ \mathrm{Prop}(k) $ を真だと仮定して $ \mathrm{Prop}(k+1) $ を証明すればいいわけですが、
$k+1$ 番目のループのあとに</p>

<ul>
  <li>$ \mathrm{vec}[p]~~(p \in [0,k]) $ が $q \in [0, k]$ である確率は、
「前回のループ後にその値であった確率」×「今回入れ替えられなかった確率」、
つまり
$$
(1 / (k + 1)) \cdot ((k + 1) / (k + 2)) = 1 / (k + 2)
$$</li>
  <li>$ \mathrm{vec}[p]~~(p \in [0,k]) $ が $k + 1$ である確率は、今回入れ替えられた確率、
つまり $1 / (k + 2)$</li>
  <li>$ \mathrm{vec}[k+1] $ が $q \in [0, k+1]$　である確率はやはり $1 / (k + 2)$</li>
</ul>

<p>というわけで  $ \mathrm{Prop}(k+1) $ が成り立ちます。</p>

<p>やっていることは実に単純ですが、ちょっと自分で思いつくのは難しそうな、
アルゴリズムらしいアルゴリズムですね。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CharRecog を GitHub にアップしました]]></title>
    <link href="http://kensak.github.io/blog/2014/01/10/uploaded-charrecog/"/>
    <updated>2014-01-10T22:20:53+09:00</updated>
    <id>http://kensak.github.io/blog/2014/01/10/uploaded-charrecog</id>
    <content type="html"><![CDATA[<p>ニューラルネットによる文字認識プログラム CharRecog を
<a href="https://github.com/kensak/CharRecog">GitHub にアップしました</a>。</p>

<h3 id="section">特徴</h3>
<ul>
  <li>C++ で作成。OpenCV の ANN_MLP クラスを参考に、コアの部分はゼロから書きなおしました。</li>
  <li>並列処理 : インテル TBB、OpenMP, C++ AMP などを使い、マルチコア CPU や GPU による並列処理をおこなっています。</li>
  <li>行列計算による高速化 : すべてのサンプル画像を処理する際、できるだけループを回さず一回の行列計算により処理をおこないます。</li>
  <li>さまざまな手法への対応 : autoencoding, convolutional layer, max pooling, maxout, dropout の各手法や
入力画像のランダムな affine 変換などに対応しています。</li>
  <li>ビルド済み実行ファイルでは float で計算をおこないますが、ソースを一ヶ所変更してリビルドすれば double にも対応します。</li>
  <li>学習された重みを画像として出力できます。</li>
</ul>

<p>すぐに使えるよう 64 ビット版のバイナリもアップしています。
MNIST 画像セットを使って autoencoding や dropout などの実験が簡単におこなえます。</p>

<p>今後も気になる手法があればインプリメントしていく予定です。
また、画像認識や言語処理のための高速なNNエンジンとしても使えるようにしていきたい。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maxout: アクティベーション関数に Max を使う]]></title>
    <link href="http://kensak.github.io/blog/2013/12/29/maxout/"/>
    <updated>2013-12-29T23:26:44+09:00</updated>
    <id>http://kensak.github.io/blog/2013/12/29/maxout</id>
    <content type="html"><![CDATA[<p>とても似た内容の二つの論文を読んでみました。
多層パーセプトロンのアクティベーション関数としてシグモイドや tanh のかわりに max を使うと文字認識などでの認識率が上がる、という話です。</p>

<!-- more -->

<p>[1] の maxout レイヤーでは、レイヤーの入力に重みを掛けてバイアスと一緒に足し合わせたあと、
従来のように tanh やシグモイドなどの関数を適用するのではなく、
出力をいくつかずつまとめてそれらの中の最大値を最終的な出力とする。</p>

<p>これは、アクティベーション関数として恒等関数を使うパーセンプトロンまたは畳み込みレイヤーと、
入力をいくつかずつ組にしてそれぞれの組の最大値のみを出力するレイヤーの２段階に分けて考えることも出来る。</p>

<p>従来の畳み込みネットでは、畳み込みレイヤーの後に max pool レイヤーを入れて、例えば入力画像をサイズが 2x2 のサブ領域に分割し、それぞれの max を出力する、
ということがおこなわれているが、やっている計算は maxout もこれと変わらない。
ただ、従来の max pool レイヤーには、
2次元イメージの特徴的なパターンのずれや回転などを吸収するという意味づけがされていたのに対し、
maxout ではもっと本質的な役割を果たしている。</p>

<p>この単純なアフィン変換と max からなるネットワークには、
十分な数のノード／レイヤーを与えられれば、従来のシグモイドや tanh と同様に、どんな連続関数でも任意の精度で近似できる能力がある。
イメージとしては、任意の連続関数はある凸関数 f, g を使って f - g と表すことができ、そして凸関数は十分な数の平面の max として近似できる、ということらしい。</p>

<p>[2] の local winner-take-all (LWTA) も maxout と似ている。
ただ、同じ組の中の最大値ではない値にも出力ノードが与えられ、これは 0 を出力するという点が違う。
しかしこの違いが重要なものかどうかはよく分からない。
LWTA を使うと一旦記憶したものを忘れにくいと説明しているが、
これは単に 0 を出力するノードにも重みが与えられていてパラメターの数が多いからかもしれない。</p>

<p>MNIST など実際のデータを使った実験では、maxout や LWTA と dropout を組み合わせた方法が従来の記録を更新する結果を出している。</p>

<p>しかし tanh の計算やバックプロパゲーション・フェーズでの grad の計算がいらなくなるのはちょっと不思議な感じがする。
今までだれもこのことを指摘しなかったのも不思議。案外こういった単純な確認が重要なのかも。
double 型の tanh の計算の並列化のためにC++ AMP を使って GPU 上で計算しようとすると、Windows 7 では double 型は制限つきサポートになっていて、
precise_math の関数が使えず割り算もできず大変だったのだが、今後は max の計算だけを考えればよいのかな。</p>

<p><strong>リファレンス</strong><br />
[1] Ian Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio. Maxout Networks. In Proceedings of the ICML, 2013.<br />
[2] Rupesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, Jurgen Schmidhuber. Compete to Compute. In Proceedings NIPS, 2013.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[初ポスト]]></title>
    <link href="http://kensak.github.io/blog/2013/12/29/first-post/"/>
    <updated>2013-12-29T21:03:36+09:00</updated>
    <id>http://kensak.github.io/blog/2013/12/29/first-post</id>
    <content type="html"><![CDATA[<p>二日がかりでようやく Github + Octopress のサイトを立ち上げました。
<!-- more --></p>

<p>数式も書けます。
$$
P(E)   = {n \choose k} p^k (1-p)^{ n-k}
$$</p>

<p>Solarized によるソースコードの表示。</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="cpp"><span class="line"><span class="c1">// For each sample (=row) in the input,</span>
</span><span class="line"><span class="c1">// &#39;dropoutRatio&#39; by ratio of columns shall be set to zero.</span>
</span><span class="line"><span class="k">static</span> <span class="kt">void</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span> <span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="o">&amp;</span><span class="n">dropped</span><span class="p">,</span> <span class="k">const</span> <span class="n">real</span> <span class="o">&amp;</span><span class="n">dropoutRatio</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">  <span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="n">mask</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">CV_8U</span><span class="p">);</span>
</span><span class="line">  <span class="n">cv</span><span class="o">::</span><span class="n">RNG</span> <span class="n">rng</span><span class="p">(</span><span class="n">time</span><span class="p">(</span><span class="nb">NULL</span><span class="p">));</span>
</span><span class="line">  <span class="n">makeDropoutMask</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dropoutRatio</span><span class="p">,</span> <span class="n">rng</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">  <span class="n">inputs</span><span class="p">.</span><span class="n">copyTo</span><span class="p">(</span><span class="n">dropped</span><span class="p">,</span> <span class="n">mask</span><span class="p">);</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="section">参考にしたサイト</h3>
<ul>
  <li>まずは<a href="http://octopress.org/docs/">Octopress 公式ドキュメント</a></li>
  <li>当方 OS が Windows 7 のため、<a href="http://stb.techelex.com/setup-octopress-on-windows7/">このサイト</a>の手順に従ってインストールしました。実は一度インストールしてから変になってしまって、Cygwin を使う方法に変更して再構築をかけたのですが泥沼にはまり、結局、再びこちらの方法に戻って現在に到っております。</li>
  <li><a href="https://github.com/stchangg/mathy">mathy というテーマ</a>は MathJax という仕組みをサポートしていてい、上のように数式を表示できるようになります。これを元に色々カスタマイズしました。</li>
  <li>静的サイトのジェネレーター<a href="http://jekyllrb.com/docs/home/">Jekyll のドキュメント</a>。octopress/sourceにユーザーが用意したファイルを元に、Jekyll が octopress/public にサイトのファイル一式を作ってくれる。Jekyll を理解しないとサイトのカスタマイズができない。</li>
</ul>

<p>その他にも多くのサイトを参考にしましたが、ほとんどは上記のリンクからたどって行けます。</p>

<h3 id="section-1">はまった点</h3>
<ul>
  <li>Github のプロファイルは、少なくとも名前と Email は前もって埋めておく。</li>
  <li>rake setup_github_pages でレポジトリの url を尋ねられるが、 git@… の方ではなく https://… の方にしておくと SSH キーを作る必要がなくなるのでおすすめ。</li>
  <li>コンソールに表示されるメッセージはよく読もう！実はエラーが起きているのに気づかなかったことがしばしば。</li>
  <li>markdown ファイルで日本語を使うときは BOM なしの UTF-8 で保存する。編集には Notepad++ を使うと便利。</li>
  <li>Solarized を使うには Python が必要となるので、Windows 用の Python を必要に応じてインストールし、Git Bash のパスに追加する必要がある。~/.bash_profile に export PATH=”/c/Ruby193/bin:/c/Python27:$PATH” という風に書いておく。</li>
</ul>

<h3 id="section-2">よく使うコマンド</h3>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class=""><span class="line">rake new_post["post title"]  # 新しいポストのソースを生成する。
</span><span class="line">rake generate    　 # サイトの静的ページを public ディレクトリに生成する。
</span><span class="line">rake preview        # 下書きを http://localhost:4000 でチェックする。
</span><span class="line">rake deploy         # public の内容をリポジトリの master に push し、サイトを更新する。
</span><span class="line">
</span><span class="line"># ソースをコミットしてリポジトリの source に push する。
</span><span class="line">git add .
</span><span class="line">git commit -m 'your comment'
</span><span class="line">git push origin source</span></code></pre></td></tr></table></div></figure></notextile></div>

]]></content>
  </entry>
  
</feed>
