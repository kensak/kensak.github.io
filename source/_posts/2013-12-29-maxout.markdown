---
layout: post
title: "ニューラルネットで tanh のかわりに max を使う"
date: 2013-12-29 23:26:44 +0900
comments: true
categories: [ニューラルネット, 論文]
---
とても似た内容の二つの論文を読んでみました。
多層パーセプトロンのアクティベーション関数としてシグモイドや tanh のかわりに max を使うと文字認識などでの認識率が上がるよ、という話です。

<!-- more -->

畳み込みネットでは、畳み込みレイヤーの後に max pool レイヤーを入れて、例えば 2x2 の領域の max を出力する、
ということがあったが、[1] の maxout という手法はその1次元版のようなもの。

つまり入力値を重みとバイアスで変換して、その結果を幾つかずつ組にしてその max を一組につき一つ出力する、
という単純な操作をおこなうのだが、
それだけでも十分な数のノード／レイヤーを使えばどんな連続関数でも任意の精度で近似できるらしい。
イメージとしては、任意の連続関数はある凸関数 f, g を使って f - g と表すことができ、そして凸関数は十分な数の平面の max として近似できる、ということらしい。

[2] の local winner-take-all (LWTA) も maxout と似ている。
ただ、同じ組の中の最大値ではない値にも出力ノードが与えられ、これは 0 を出力するという点が違う。
しかしこの違いが重要なものかどうかはよく分からない。
LWTA を使うと一旦記憶したものを忘れにくいと説明しているが、
これは単に 0 を出力するノードにも重みが与えられていてパラメターの数が多いからかもしれない。

MNIST など実際のデータを使った実験では、maxout や LWTA と dropout を組み合わせた方法が従来の記録を更新する結果を出している。

しかし tanh の計算やバックプロパゲーション・フェーズでの grad の計算がいらなくなるのはちょっと不思議な感じがする。
今までだれもこのことを指摘しなかったのも不思議。案外こういった単純な確認が重要なのかも。
double 型の tanh の計算の並列化のためにC++ AMP を使って GPU 上で計算しようとすると、Windows 7 では double 型は制限つきサポートになっていて、
precise_math の関数が使えず割り算もできず大変だったのだが、今後は max の計算だけを考えればよいのかな。

**リファレンス**<br>
[1] Ian Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio. Maxout Networks. In Proceedings of the ICML, 2013.<br>
[2] Rupesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, Jurgen Schmidhuber. Compete to Compute. In Proceedings NIPS, 2013.