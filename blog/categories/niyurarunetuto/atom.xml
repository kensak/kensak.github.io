<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ニューラルネット | AIへ秒読み]]></title>
  <link href="http://kensak.github.io/blog/categories/niyurarunetuto/atom.xml" rel="self"/>
  <link href="http://kensak.github.io/"/>
  <updated>2014-01-09T15:16:37+09:00</updated>
  <id>http://kensak.github.io/</id>
  <author>
    <name><![CDATA[榊原 研]]></name>
    <email><![CDATA[ken.sakakibar@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[ニューラルネットでのオーバーフローについてのメモ]]></title>
    <link href="http://kensak.github.io/blog/2014/01/08/linear-perceptron-overflow/"/>
    <updated>2014-01-08T22:03:03+09:00</updated>
    <id>http://kensak.github.io/blog/2014/01/08/linear-perceptron-overflow</id>
    <content type="html"><![CDATA[<p>前回はニューラルネットの新たな（？）手法、maxout についてふれました。
これは tanh やシグモイドなどのアクティベーション関数を使うかわりに、
出力をいくつかづつにまとめてその max を計算し、レイヤーの出力とするというものでした。</p>

<p>この maxout レイヤーを複数重ねる場合、そのままでは重みが大きくなることによる浮動小数点の計算エラーが発生することがあります。
つまり、maxout では入力に重みを掛けるばかりで tanh のように値域を狭めるものがないので、出力が爆発してしまう。</p>

<p>これを避けるには、[1]にあるように重み行列のノルムに制限をかけ、制限を越えたら重みとバイアスに同じ係数を掛けてスケールダウンする必要があります。</p>

<!-- more -->

<p>これはOpenCVを使えばこんな風に書けます。
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='cpp'><span class='line'><span class="kt">double</span> <span class="n">weightNorm</span> <span class="o">=</span> <span class="n">cv</span><span class="o">::</span><span class="n">norm</span><span class="p">(</span><span class="n">weight</span><span class="p">);</span>
</span><span class='line'><span class="k">if</span> <span class="p">(</span><span class="n">maxWeightNorm</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="n">weightNorm</span><span class="p">)</span>
</span><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="kt">double</span> <span class="n">coef</span> <span class="o">=</span> <span class="n">maxWeightNorm</span> <span class="o">/</span> <span class="n">weightNorm</span><span class="p">;</span>
</span><span class='line'>  <span class="n">weight</span> <span class="o">*=</span> <span class="n">coef</span><span class="p">;</span>
</span><span class='line'>  <span class="n">bias</span> <span class="o">*=</span> <span class="n">coef</span><span class="p">;</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>また、これは maxout に限りませんが、恒等関数をアクティベーション関数とするレイヤーの出力をそのまま softmax レイヤーに接続する場合、
入力が大きすぎて指数関数がオーバーフローすることがあります。</p>

<p>softmax のアクティベーション $\mathbf{a}$ は入力 $\mathbf{x}$ より
$$
a_i = \frac{\exp(x_i)}{\sum_k \exp(x_k)}
$$
で計算されますから、$\mathbf{x}$ のすべての要素から同じ数をさっぴいても結果は変わりません。
要素の max を計算し、それをすべての要素から引いてから計算することで、#INF00 を避けることができます。</p>

<p><strong>リファレンス</strong><br />
[1] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R.　Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors.
2012. arXiv:1207.0580.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maxout: アクティベーション関数に Max を使う]]></title>
    <link href="http://kensak.github.io/blog/2013/12/29/maxout/"/>
    <updated>2013-12-29T23:26:44+09:00</updated>
    <id>http://kensak.github.io/blog/2013/12/29/maxout</id>
    <content type="html"><![CDATA[<p>とても似た内容の二つの論文を読んでみました。
多層パーセプトロンのアクティベーション関数としてシグモイドや tanh のかわりに max を使うと文字認識などでの認識率が上がる、という話です。</p>

<!-- more -->

<p>[1] の maxout レイヤーでは、レイヤーの入力に重みを掛けてバイアスと一緒に足し合わせたあと、
従来のように tanh やシグモイドなどの関数を適用するのではなく、
出力をいくつかずつまとめてそれらの中の最大値を最終的な出力とする。</p>

<p>これは、アクティベーション関数として恒等関数を使うパーセンプトロンまたは畳み込みレイヤーと、
入力をいくつかずつ組にしてそれぞれの組の最大値のみを出力するレイヤーの２段階に分けて考えることも出来る。</p>

<p>従来の畳み込みネットでは、畳み込みレイヤーの後に max pool レイヤーを入れて、例えば入力画像をサイズが 2x2 のサブ領域に分割し、それぞれの max を出力する、
ということがおこなわれているが、やっている計算は maxout もこれと変わらない。
ただ、従来の max pool レイヤーには、
2次元イメージの特徴的なパターンのずれや回転などを吸収するという意味づけがされていたのに対し、
maxout ではもっと本質的な役割を果たしている。</p>

<p>この単純なアフィン変換と max からなるネットワークには、
十分な数のノード／レイヤーを与えられれば、従来のシグモイドや tanh と同様に、どんな連続関数でも任意の精度で近似できる能力がある。
イメージとしては、任意の連続関数はある凸関数 f, g を使って f - g と表すことができ、そして凸関数は十分な数の平面の max として近似できる、ということらしい。</p>

<p>[2] の local winner-take-all (LWTA) も maxout と似ている。
ただ、同じ組の中の最大値ではない値にも出力ノードが与えられ、これは 0 を出力するという点が違う。
しかしこの違いが重要なものかどうかはよく分からない。
LWTA を使うと一旦記憶したものを忘れにくいと説明しているが、
これは単に 0 を出力するノードにも重みが与えられていてパラメターの数が多いからかもしれない。</p>

<p>MNIST など実際のデータを使った実験では、maxout や LWTA と dropout を組み合わせた方法が従来の記録を更新する結果を出している。</p>

<p>しかし tanh の計算やバックプロパゲーション・フェーズでの grad の計算がいらなくなるのはちょっと不思議な感じがする。
今までだれもこのことを指摘しなかったのも不思議。案外こういった単純な確認が重要なのかも。
double 型の tanh の計算の並列化のためにC++ AMP を使って GPU 上で計算しようとすると、Windows 7 では double 型は制限つきサポートになっていて、
precise_math の関数が使えず割り算もできず大変だったのだが、今後は max の計算だけを考えればよいのかな。</p>

<p><strong>リファレンス</strong><br />
[1] Ian Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio. Maxout Networks. In Proceedings of the ICML, 2013.<br />
[2] Rupesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, Jurgen Schmidhuber. Compete to Compute. In Proceedings NIPS, 2013.</p>
]]></content>
  </entry>
  
</feed>
